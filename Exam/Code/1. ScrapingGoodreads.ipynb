{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2a58a2-afd8-4978-a21b-3b8cbe298c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "fp_data = Path.cwd() / \"data\"  \n",
    "# Use the Path object to actually create the subfolder\n",
    "Path.mkdir(fp_data, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eac05db-86a1-4212-b339-a3872944ec5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrabing data about books on the 5 social sciences from Goodreads.com by looping over the URLs \n",
    "\n",
    "html=[]\n",
    "\n",
    "subjects = ['economics', 'sociology', 'anthropology', 'psychology', 'political+science']\n",
    "\n",
    "for page in tqdm.tqdm(range(1,101)):\n",
    "    for subject in subjects:\n",
    "        url = f'https://www.goodreads.com/search?page={page}&utf8=%E2%9C%93&query={subject}'\n",
    "        response = requests.get(url, headers={'name':'Anna Nielsen', 'Universitet': 'UNI of Copenhagen','Message': 'Data collecting for exam project in Social Data science', 'email':'djb713@alumni.ku.dk'})\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        a_links = soup.find_all('a', class_='bookTitle')\n",
    "        html.append(a_links)\n",
    "    \n",
    "        time.sleep(5)\n",
    "    \n",
    "html2 = [item for sublist in html for item in sublist]\n",
    "\n",
    "liste2=[]\n",
    "for i in html2:\n",
    "    a_links2=i.get('href')\n",
    "    liste2.append('https://www.goodreads.com'+a_links2)\n",
    "    \n",
    "len(liste2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1dae21-bf8b-47a1-98ae-7119aa368719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping dublicates from the list of URLs above \n",
    "\n",
    "list_nodup = []\n",
    "\n",
    "for item in liste2:\n",
    "    if item not in list_nodup:\n",
    "        list_nodup.append(item)\n",
    "\n",
    "len(list_nodup)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879da832-46d3-4049-98db-b0b545201f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log(response, logfile, output_path=fp_data):\n",
    "    # Create the local folder if it doesn't exist\n",
    "    if not os.path.exists(fp_data):\n",
    "        os.makedirs(fp_data)\n",
    "    \n",
    "    # Construct the full path for the log file within the fp_data folder\n",
    "    log_filepath = os.path.join(fp_data, logfile)\n",
    "    \n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile(log_filepath):\n",
    "        log_mode = 'a'\n",
    "    else:\n",
    "        log_mode = 'w'\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file']\n",
    "        with open(log_filepath, 'w') as log:\n",
    "            log.write(';'.join(header) + '\\n')\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response2.status_code\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "    length = len(response2.text)\n",
    "    \n",
    "    # Append the gathered log information\n",
    "    with open(log_filepath, 'a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{log_filepath}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae01ab24-33f6-4183-905e-522bd91206b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Storing data on each book in a dataframe 'books_df'. \n",
    "#We loop over 5 attemps to try getting the information, else we drop the book \n",
    "#With try and except we drop books with errors in loading information (often because the book don't have the variable we are interested in)\n",
    "\n",
    "books_df = pd.DataFrame()\n",
    "\n",
    "loops = 0\n",
    "\n",
    "logfile = 'log.csv'\n",
    "\n",
    "for url in tqdm.tqdm(list_nodup):\n",
    "    \n",
    "    loops += 1\n",
    "    #print(loops)\n",
    "        \n",
    "    try:\n",
    "        attempts = 5\n",
    "        for _ in range(attempts):\n",
    "            \n",
    "            response2 = requests.get(url, headers={'name':'Barbara Bille Tagmose', 'Universitet': 'UNI of Copenhagen','Besked': 'Data collecting for exam project in Social Data science', 'email':'wjl145@alumni.ku.dk'})\n",
    "            time.sleep(5)\n",
    "            suppe = BeautifulSoup(response2.content, 'lxml') \n",
    "\n",
    "            #Finder elementer i supper \n",
    "            bogtitel_ = suppe.find_all('h1', class_='Text Text__title1')\n",
    "            stjerner_ = suppe.find_all('div', class_='RatingStatistics__rating')\n",
    "            metastatistik_ = suppe.find_all('div', class_='RatingStatistics__meta')\n",
    "            forfattere_ = suppe.find_all('div', class_='ContributorLinksList')\n",
    "            beskrivelse_ = suppe.find_all('div', class_='BookPageMetadataSection__description')\n",
    "            plublishdetails_ = suppe.find_all('div', class_='FeaturedDetails')\n",
    "\n",
    "            #Laver tomme lister \n",
    "            bogtitel = []\n",
    "            Stjerner = []\n",
    "            metastatistik = []\n",
    "            nratings = []\n",
    "            nreviews =[]\n",
    "            forfatter = []\n",
    "            beskrivelse = []\n",
    "            plublishdetails = []\n",
    "            pages = []\n",
    "            published = []\n",
    "\n",
    "            #Laver variable i loops \n",
    "            for i in bogtitel_:\n",
    "                bogtitel = str(i.text)\n",
    "\n",
    "            for i in stjerner_:\n",
    "                Stjerner = float(i.text)\n",
    "\n",
    "            for i in metastatistik_:\n",
    "                metastatistik = i.find_all('span')\n",
    "\n",
    "            for idx, r in enumerate(metastatistik):\n",
    "                if idx == 0:\n",
    "                    ratingsstr = str(r)\n",
    "                    nratings = int(re.sub(r'\\D+','',ratingsstr))\n",
    "                else:\n",
    "                    reviewsstr = str(r)\n",
    "                    nreviews = int(re.sub(r'\\D+','',reviewsstr))\n",
    "\n",
    "            for i in forfattere_:\n",
    "                forfatter.append(i.text)\n",
    "\n",
    "            for i in beskrivelse_:\n",
    "                beskrivelse = str(i.text)\n",
    "\n",
    "            for i in plublishdetails_:\n",
    "                plublishdetails = i.find_all('p')\n",
    "\n",
    "            for idx, i in enumerate(plublishdetails):\n",
    "                if idx < len(plublishdetails)-1:\n",
    "                    pagesstr = str(i.text)\n",
    "                    pages = int(re.sub(r'\\D+','',pagesstr))\n",
    "                else:\n",
    "                    publishedstr = str(i.text)\n",
    "                    publisheddig = str(re.sub(r'\\D+','',publishedstr))\n",
    "                    match = re.search(r'.{4}$', publisheddig)\n",
    "                    published = int(match.group())\n",
    "\n",
    "            #Laver om til et pandas dataframe \n",
    "\n",
    "            data = {\n",
    "            'Title': bogtitel,\n",
    "            'Author': forfatter,\n",
    "            'Avg_ratings': Stjerner,\n",
    "            'nratings': nratings,\n",
    "            'nreviews': nreviews,\n",
    "            'Description': beskrivelse,\n",
    "            'Pages': pages,\n",
    "            'Published_year': published\n",
    "\n",
    "            }\n",
    "            #print(data)\n",
    "\n",
    "            result_df = pd.DataFrame(data)\n",
    "            \n",
    "            log(response2,logfile)\n",
    "\n",
    "\n",
    "            if not result_df.empty:\n",
    "                books_df = pd.concat([books_df,result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "                \n",
    "                if loops %100 == 0:\n",
    "                    books_df.to_csv(fp_data/'bookssave100.csv')\n",
    "                \n",
    "                break    \n",
    "            #else:\n",
    "                #print('fejl')\n",
    "        #else:\n",
    "            #print('overskredet')\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
