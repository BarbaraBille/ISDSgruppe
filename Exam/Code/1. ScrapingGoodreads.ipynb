{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2a58a2-afd8-4978-a21b-3b8cbe298c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "fp_data = Path.cwd() / \"data\"  \n",
    "# Use the Path object to actually create the subfolder\n",
    "Path.mkdir(fp_data, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eac05db-86a1-4212-b339-a3872944ec5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [57:33<00:00, 34.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9979"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrabing data about books on the 5 social sciences from Goodreads.com by looping over the URLs \n",
    "\n",
    "html=[]\n",
    "\n",
    "subjects = ['economics', 'sociology', 'anthropology', 'psychology', 'political+science']\n",
    "\n",
    "for page in tqdm.tqdm(range(1,101)):\n",
    "    for subject in subjects:\n",
    "        url = f'https://www.goodreads.com/search?page={page}&utf8=%E2%9C%93&query={subject}'\n",
    "        response = requests.get(url, headers={'name':'Anna Nielsen', 'Universitet': 'UNI of Copenhagen','Message': 'Data collecting for exam project in Social Data science', 'email':'djb713@alumni.ku.dk'})\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        a_links = soup.find_all('a', class_='bookTitle')\n",
    "        html.append(a_links)\n",
    "    \n",
    "        time.sleep(5)\n",
    "    \n",
    "html2 = [item for sublist in html for item in sublist]\n",
    "\n",
    "liste2=[]\n",
    "for i in html2:\n",
    "    a_links2=i.get('href')\n",
    "    liste2.append('https://www.goodreads.com'+a_links2)\n",
    "    \n",
    "len(liste2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1dae21-bf8b-47a1-98ae-7119aa368719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9979"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping dublicates from the list of URLs above \n",
    "\n",
    "list_nodup = []\n",
    "\n",
    "for item in liste2:\n",
    "    if item not in list_nodup:\n",
    "        list_nodup.append(item)\n",
    "\n",
    "len(list_nodup)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effa7f5-4b90-45d0-a5da-263d356d0465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [08:27<03:39,  6.85s/it]"
     ]
    }
   ],
   "source": [
    "# KUN ØKONOMI \n",
    "\n",
    "html=[]\n",
    "\n",
    "\n",
    "for page in tqdm.tqdm(range(1,101)):\n",
    "    url = f'https://www.goodreads.com/search?page={page}&utf8=%E2%9C%93&query=economics'\n",
    "    response = requests.get(url, headers={'name':'Anna Nielsen', 'Universitet': 'UNI of Copenhagen','Message': 'Data collecting for exam project in Social Data science', 'email':'djb713@alumni.ku.dk'})\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    a_links = soup.find_all('a', class_='bookTitle')\n",
    "    html.append(a_links)\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "listlist=[]\n",
    "for i in html:\n",
    "    a_links2=i.get('href')\n",
    "    listlist.append('https://www.goodreads.com'+a_links2)\n",
    "    \n",
    "len(listlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879da832-46d3-4049-98db-b0b545201f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log(response, logfile, output_path=fp_data):\n",
    "    # Create the local folder if it doesn't exist\n",
    "    if not os.path.exists(fp_data):\n",
    "        os.makedirs(fp_data)\n",
    "    \n",
    "    # Construct the full path for the log file within the fp_data folder\n",
    "    log_filepath = os.path.join(fp_data, logfile)\n",
    "    \n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile(log_filepath):\n",
    "        log_mode = 'a'\n",
    "    else:\n",
    "        log_mode = 'w'\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file']\n",
    "        with open(log_filepath, 'w') as log:\n",
    "            log.write(';'.join(header) + '\\n')\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response2.status_code\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "    length = len(response2.text)\n",
    "    \n",
    "    # Append the gathered log information\n",
    "    with open(log_filepath, 'a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{log_filepath}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae01ab24-33f6-4183-905e-522bd91206b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/9979 [11:41<43:02:42, 15.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(attempts):\n\u001b[1;32m     20\u001b[0m     response2 \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBarbara Bille Tagmose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniversitet\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNI of Copenhagen\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBesked\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData collecting for exam project in Social Data science\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwjl145@alumni.ku.dk\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     suppe \u001b[38;5;241m=\u001b[39m BeautifulSoup(response2\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#Finder elementer i supper \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Storing data on each book in a dataframe 'books_df'. \n",
    "#We loop over 5 attemps to try getting the information, else we drop the book \n",
    "#With try and except we drop books with errors in loading information (often because the book don't have the variable we are interested in)\n",
    "\n",
    "books_df = pd.DataFrame()\n",
    "\n",
    "loops = 0\n",
    "\n",
    "logfile = 'log.csv'\n",
    "\n",
    "for url in tqdm.tqdm(listlist):\n",
    "    \n",
    "    loops += 1\n",
    "    #print(loops)\n",
    "        \n",
    "    try:\n",
    "        attempts = 5\n",
    "        for _ in range(attempts):\n",
    "            \n",
    "            response2 = requests.get(url, headers={'name':'Barbara Bille Tagmose', 'Universitet': 'UNI of Copenhagen','Besked': 'Data collecting for exam project in Social Data science', 'email':'wjl145@alumni.ku.dk'})\n",
    "            time.sleep(5)\n",
    "            suppe = BeautifulSoup(response2.content, 'lxml') \n",
    "\n",
    "            #Finder elementer i supper \n",
    "            bogtitel_ = suppe.find_all('h1', class_='Text Text__title1')\n",
    "            stjerner_ = suppe.find_all('div', class_='RatingStatistics__rating')\n",
    "            metastatistik_ = suppe.find_all('div', class_='RatingStatistics__meta')\n",
    "            forfattere_ = suppe.find_all('div', class_='ContributorLinksList')\n",
    "            beskrivelse_ = suppe.find_all('div', class_='BookPageMetadataSection__description')\n",
    "            plublishdetails_ = suppe.find_all('div', class_='FeaturedDetails')\n",
    "\n",
    "            #Laver tomme lister \n",
    "            bogtitel = []\n",
    "            Stjerner = []\n",
    "            metastatistik = []\n",
    "            nratings = []\n",
    "            nreviews =[]\n",
    "            forfatter = []\n",
    "            beskrivelse = []\n",
    "            plublishdetails = []\n",
    "            pages = []\n",
    "            published = []\n",
    "\n",
    "            #Laver variable i loops \n",
    "            for i in bogtitel_:\n",
    "                bogtitel = str(i.text)\n",
    "\n",
    "            for i in stjerner_:\n",
    "                Stjerner = float(i.text)\n",
    "\n",
    "            for i in metastatistik_:\n",
    "                metastatistik = i.find_all('span')\n",
    "\n",
    "            for idx, r in enumerate(metastatistik):\n",
    "                if idx == 0:\n",
    "                    ratingsstr = str(r)\n",
    "                    nratings = int(re.sub(r'\\D+','',ratingsstr))\n",
    "                else:\n",
    "                    reviewsstr = str(r)\n",
    "                    nreviews = int(re.sub(r'\\D+','',reviewsstr))\n",
    "\n",
    "            for i in forfattere_:\n",
    "                forfatter.append(i.text)\n",
    "\n",
    "            for i in beskrivelse_:\n",
    "                beskrivelse = str(i.text)\n",
    "\n",
    "            for i in plublishdetails_:\n",
    "                plublishdetails = i.find_all('p')\n",
    "\n",
    "            for idx, i in enumerate(plublishdetails):\n",
    "                if idx < len(plublishdetails)-1:\n",
    "                    pagesstr = str(i.text)\n",
    "                    pages = int(re.sub(r'\\D+','',pagesstr))\n",
    "                else:\n",
    "                    publishedstr = str(i.text)\n",
    "                    publisheddig = str(re.sub(r'\\D+','',publishedstr))\n",
    "                    match = re.search(r'.{4}$', publisheddig)\n",
    "                    published = int(match.group())\n",
    "\n",
    "            #Laver om til et pandas dataframe \n",
    "\n",
    "            data = {\n",
    "            'Title': bogtitel,\n",
    "            'Author': forfatter,\n",
    "            'Avg_ratings': Stjerner,\n",
    "            'nratings': nratings,\n",
    "            'nreviews': nreviews,\n",
    "            'Description': beskrivelse,\n",
    "            'Pages': pages,\n",
    "            'Published_year': published\n",
    "\n",
    "            }\n",
    "            #print(data)\n",
    "\n",
    "            result_df = pd.DataFrame(data)\n",
    "            \n",
    "            log(response2,logfile)\n",
    "\n",
    "\n",
    "            if not result_df.empty:\n",
    "                books_df = pd.concat([books_df,result_df], axis=0, ignore_index=True) #Append to the rest of the data\n",
    "                \n",
    "                if loops %100 == 0:\n",
    "                    books_df.to_csv(fp_data/'bookssave100.csv')\n",
    "                \n",
    "                break    \n",
    "            #else:\n",
    "                #print('fejl')\n",
    "        #else:\n",
    "            #print('overskredet')\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "        continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e914645-cb3a-4a3a-9e06-c105a7c355cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Avg_ratings</th>\n",
       "      <th>nratings</th>\n",
       "      <th>nreviews</th>\n",
       "      <th>Description</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Published_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Poor Economics: A Radical Rethinking of the Wa...</td>\n",
       "      <td>Abhijit V. Banerjee, Esther Duflo</td>\n",
       "      <td>4.28</td>\n",
       "      <td>20752</td>\n",
       "      <td>1888</td>\n",
       "      <td>Why do the poor borrow to save? Why do they mi...</td>\n",
       "      <td>320</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Misbehaving: The Making of Behavioral Economics</td>\n",
       "      <td>Richard H. Thaler</td>\n",
       "      <td>4.16</td>\n",
       "      <td>20440</td>\n",
       "      <td>1523</td>\n",
       "      <td>Nobel laureate Richard H. Thaler has spent his...</td>\n",
       "      <td>358</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good Economics for Hard Times: Better Answers ...</td>\n",
       "      <td>Abhijit V. Banerjee, Esther Duflo</td>\n",
       "      <td>4.24</td>\n",
       "      <td>12174</td>\n",
       "      <td>1262</td>\n",
       "      <td>Figuring out how to deal with today's critical...</td>\n",
       "      <td>417</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Poor Economics: A Radical Rethinking of the Wa...   \n",
       "1    Misbehaving: The Making of Behavioral Economics   \n",
       "2  Good Economics for Hard Times: Better Answers ...   \n",
       "\n",
       "                              Author  Avg_ratings  nratings  nreviews  \\\n",
       "0  Abhijit V. Banerjee, Esther Duflo         4.28     20752      1888   \n",
       "1                  Richard H. Thaler         4.16     20440      1523   \n",
       "2  Abhijit V. Banerjee, Esther Duflo         4.24     12174      1262   \n",
       "\n",
       "                                         Description  Pages  Published_year  \n",
       "0  Why do the poor borrow to save? Why do they mi...    320            2011  \n",
       "1  Nobel laureate Richard H. Thaler has spent his...    358            2016  \n",
       "2  Figuring out how to deal with today's critical...    417            2019  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde21a2-0672-4e53-bf43-06f590425850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
